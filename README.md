# Transformer-from-scratch
This repository contains a complete, from-scratch implementation of a Transformer Encoder model in PyTorch. The model is built to perform text classification (specifically, sentiment analysis) on the IMDB movie review dataset.
The rise of models like BERT and GPT is entirely thanks to the Transformer. While it's easy to import a pre-built Transformer, the best way to truly understand how it works is to build one.This project is a step-by-step implementation of an Encoder-Only Transformer, similar to the architecture used by classic models like BERT for text understanding. This model is built in PyTorch based only on the mechanisms described in the original 2017 paper, "Attention Is All You Need".Instead of using nn.Transformer, this code manually implements all the core components to demonstrate a deep, practical understanding of modern NLP architecture.Key Components Built from ScratchThis implementation includes all the essential components of the Transformer Encoder, built as individual nn.Module classes:PositionalEncoding: The sin/cos formula to inject sequence order.ScaledDotProductAttention: The core $softmax(\frac{QK^T}{\sqrt{d_k}})V$ math.MultiHeadAttention: The main module that manages 8 parallel attention "heads" and their trainable w_q, w_k, w_v linear projections.PositionwiseFeedForward: The "expand-and-contract" processing sub-layer.EncoderLayer: The complete block that assembles the two sub-layers with "Add & Norm" (residual connections and layer normalization).Encoder: The final stack of $N$ EncoderLayer blocks.How to RunThe entire project is contained within the .ipynb notebook. The easiest way to run it is:Open the notebook file directly in Google Colab.Run the cells in order from top to bottom.The notebook will automatically install datasets and transformers (for the tokenizer), download and preprocess the IMDB data, and begin training the model on a free GPU.ðŸ“ˆ ResultsThe model was trained for 3 epochs on a 5,000-sample subset of the IMDB dataset.Final Test Accuracy: [Your Accuracy]%
