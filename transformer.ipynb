{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveena0506/Transformer-from-scratch/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGpIvWBDJMWm"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np # You'll likely need it later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oI0TONWJhjS"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Transformer Building Blocks (The \"Blueprint\")\n",
        "\n",
        "# --- 1. Positional Encoding ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# --- 2. Scaled Dot-Product Attention (Helper Function) ---\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
        "    d_k = q.size(-1)\n",
        "    scaled_attention_scores = matmul_qk / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled_attention_scores = scaled_attention_scores.masked_fill(mask == 0, -1e9)\n",
        "    attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "# --- 3. Multi-Head Attention (The Main Module) ---\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.h, self.d_k)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def forward(self, q_in, k_in, v_in, mask):\n",
        "        batch_size = q_in.size(0)\n",
        "        q = self.w_q(q_in)\n",
        "        k = self.w_k(k_in)\n",
        "        v = self.w_v(v_in)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        context, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous()\n",
        "        context = context.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.w_o(context)\n",
        "        return output\n",
        "  # Cell 2 (Continued): Add this code\n",
        "\n",
        "# --- 4. Position-wise Feed-Forward Network ---\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the FFN module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimension of the model (e.g., 512).\n",
        "            d_ff (int): The inner-layer dimension, usually 4*d_model (e.g., 2048).\n",
        "            dropout (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # The paper uses d_ff = 2048 for a d_model = 512\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the FFN.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor. Shape: [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor. Shape: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # 1. Pass through the first linear layer and ReLU\n",
        "        #    [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff]\n",
        "        intermediate = self.relu(self.w_1(x))\n",
        "\n",
        "        # 2. Apply dropout\n",
        "        intermediate = self.dropout(intermediate)\n",
        "\n",
        "        # 3. Pass through the second linear layer\n",
        "        #    [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n",
        "        output = self.w_2(intermediate)\n",
        "\n",
        "        return output\n",
        "# Cell 2 (Continued): Add this code\n",
        "\n",
        "# --- 5. The Full Encoder Layer ---\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a single Encoder Layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimension of the model (e.g., 512).\n",
        "            h (int): The number of attention heads (e.g., 8).\n",
        "            d_ff (int): The inner-layer dimension of the FFN (e.g., 2048).\n",
        "            dropout (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # --- Sub-layer 1: Multi-Head Attention ---\n",
        "        self.self_attn = MultiHeadAttention(d_model, h)\n",
        "        # We need Layer Normalization and Dropout for the residual connection\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # --- Sub-layer 2: Position-wise Feed-Forward ---\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor. Shape: [batch_size, seq_len, d_model]\n",
        "            mask (torch.Tensor): The mask for self-attention.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor. Shape: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Process Sub-layer 1 (Self-Attention) ---\n",
        "\n",
        "        # 1. Save the original input for the residual connection\n",
        "        residual1 = x\n",
        "\n",
        "        # 2. Pass the input through the attention layer\n",
        "        #    Note: In self-attention, q, k, and v are all from the same input 'x'\n",
        "        attn_output = self.self_attn(q_in=x, k_in=x, v_in=x, mask=mask)\n",
        "\n",
        "        # 3. Apply dropout and add the residual connection\n",
        "        x = self.dropout1(attn_output)\n",
        "        x = x + residual1\n",
        "\n",
        "        # 4. Apply layer normalization\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # --- Process Sub-layer 2 (Feed-Forward) ---\n",
        "\n",
        "        # 1. Save the intermediate input for the residual connection\n",
        "        residual2 = x\n",
        "\n",
        "        # 2. Pass the input through the feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # 3. Apply dropout and add the residual connection\n",
        "        x = self.dropout2(ffn_output)\n",
        "        x = x + residual2\n",
        "\n",
        "        # 4. Apply layer normalization\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "  # Cell 2 (Continued): Add this code\n",
        "\n",
        "# --- 6. The Full Encoder ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, h, d_ff, N, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the full Encoder stack.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Model dimension (e.g., 512).\n",
        "            h (int): Number of heads (e.g., 8).\n",
        "            d_ff (int): FFN inner dimension (e.g., 2048).\n",
        "            N (int): Number of EncoderLayer blocks to stack (e.g., 6).\n",
        "            dropout (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Create a list of N EncoderLayer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, h, d_ff, dropout) for _ in range(N)\n",
        "        ])\n",
        "        # Add a final LayerNorm\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the full Encoder.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input. Shape: [batch_size, seq_len, d_model]\n",
        "            mask (torch.Tensor): The mask for self-attention.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output. Shape: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Pass the input through each layer in the stack\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Apply the final layer normalization\n",
        "        return self.norm(x)\n",
        "  # Cell 2 (Continued): Add this code\n",
        "\n",
        "# --- 7. The Full Transformer Classifier ---\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, h, d_ff, N, num_classes, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the complete Transformer-based classifier.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of your vocabulary (e.g., 30000).\n",
        "            d_model (int): Model dimension (e.g., 512).\n",
        "            h (int): Number of heads (e.g., 8).\n",
        "            d_ff (int): FFN inner dimension (e.g., 2048).\n",
        "            N (int): Number of EncoderLayer blocks (e.g., 6).\n",
        "            num_classes (int): The number of output classes (e.g., 2 for pos/neg).\n",
        "            dropout (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # --- 1. Input Embedding ---\n",
        "        # This layer learns the \"meaning\" vector for each word in your vocab\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # --- 2. Positional Encoding ---\n",
        "        # This adds the \"position\" vector\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # --- 3. Full Encoder Stack ---\n",
        "        # This is the main \"brain\" of the model\n",
        "        self.encoder = Encoder(d_model, h, d_ff, N, dropout)\n",
        "\n",
        "        # --- 4. The Classifier Head ---\n",
        "        # This takes the final output and turns it into a class prediction\n",
        "        self.classifier_head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_input, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the classifier.\n",
        "\n",
        "        Args:\n",
        "            x_input (torch.Tensor): Input token IDs. Shape: [batch_size, seq_len]\n",
        "            mask (torch.Tensor): The padding mask. Shape: [batch_size, 1, seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The final class logits. Shape: [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # --- 1. Get Embeddings & Add Position ---\n",
        "        # x_input shape: [batch_size, seq_len]\n",
        "\n",
        "        # Get word embeddings\n",
        "        x = self.embedding(x_input) # Shape: [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Scale embeddings (as done in the paper)\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # --- 2. Pass through Encoder ---\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        x = self.encoder(x, mask)\n",
        "\n",
        "        # --- 3. Get Classification ---\n",
        "        # To classify the whole sentence, we can just average all\n",
        "        # the word vectors from the final layer.\n",
        "        # This is a simple and effective method.\n",
        "        # x.mean(dim=1) takes the average across the seq_len dimension.\n",
        "        # Shape: [batch_size, seq_len, d_model] -> [batch_size, d_model]\n",
        "        x_pooled = x.mean(dim=1)\n",
        "\n",
        "        # Pass the pooled output through the final linear layer\n",
        "        # Shape: [batch_size, d_model] -> [batch_size, num_classes]\n",
        "        output = self.classifier_head(x_pooled)\n",
        "\n",
        "        return output\n",
        "# Cell 2 (Continued): Add this new function\n",
        "\n",
        "def create_padding_mask(input_ids, pad_token_id):\n",
        "    \"\"\"\n",
        "    Creates the padding mask for the Encoder.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Input token IDs. Shape: [batch_size, seq_len]\n",
        "        pad_token_id (int): The ID of the padding token.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The padding mask.\n",
        "                      Shape: [batch_size, 1, 1, seq_len]\n",
        "    \"\"\"\n",
        "    # Find where the input_ids are NOT the padding token\n",
        "    # (batch_size, seq_len)\n",
        "    mask = (input_ids != pad_token_id)\n",
        "\n",
        "    # Reshape to the broadcastable shape for attention\n",
        "    # (batch_size, 1, 1, seq_len)\n",
        "    return mask.unsqueeze(1).unsqueeze(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnlOPny-JpUH"
      },
      "outputs": [],
      "source": [
        "# Cell 3 (Continued): Add this new test\n",
        "\n",
        "print(\"\\n--- Testing final model ---\")\n",
        "\n",
        "# --- Define Model Hyperparameters ---\n",
        "vocab_size = 30000  # How many unique words in our dictionary\n",
        "num_classes = 2     # 2 classes: positive and negative\n",
        "d_model = 512\n",
        "h = 8\n",
        "d_ff = 2048\n",
        "N = 6               # The paper uses 6 layers\n",
        "\n",
        "# --- Create a mock input tensor ---\n",
        "batch_size = 3\n",
        "seq_len = 10\n",
        "# This time, the input is token IDs (just integers), not vectors\n",
        "mock_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "print(f\"Mock input IDs shape: {mock_input_ids.shape}\")\n",
        "\n",
        "# Create the same simple mask\n",
        "mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "# --- 1. Test the Full TransformerClassifier ---\n",
        "model = TransformerClassifier(vocab_size, d_model, h, d_ff, N, num_classes)\n",
        "\n",
        "# Run the input through the full model\n",
        "final_output = model(mock_input_ids, mask)\n",
        "\n",
        "print(f\"Final model output shape: {final_output.shape}\")\n",
        "\n",
        "# --- Check the final output shape ---\n",
        "expected_shape = torch.Size([batch_size, num_classes])\n",
        "assert final_output.shape == expected_shape\n",
        "\n",
        "print(\"\\nSuccess! Your full Transformer Classifier is built and working correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0mLMG--nE3C"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Load Data and Tokenizer\n",
        "!pip install datasets transformers\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. Load a Tokenizer ---\n",
        "# We use a pre-trained tokenizer. Don't build this from scratch.\n",
        "# \"bert-base-uncased\" is a good, standard tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# --- 2. Create a Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text, pad it, and truncate it to 512 tokens\n",
        "    return tokenizer(examples[\"text\"],\n",
        "                     padding=\"max_length\",  # Pad to max_length\n",
        "                     truncation=True,       # Truncate to max_length\n",
        "                     max_length=512)        # Standard max length\n",
        "\n",
        "# --- 3. Load the IMDB Dataset ---\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# This will download the dataset and cache it\n",
        "# We only take 5000 examples from train/test to make it run FAST\n",
        "dataset = load_dataset(\"imdb\", split={\n",
        "    'train': 'train[:5000]',\n",
        "    'test': 'test[:5000]'\n",
        "})\n",
        "\n",
        "# --- 4. Apply the Preprocessing ---\n",
        "print(\"Tokenizing and preprocessing data...\")\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# --- 5. Set Format and Create DataLoaders ---\n",
        "# This converts the dataset from \"Hugging Face format\" to \"PyTorch format\"\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Set up our training and testing DataLoaders\n",
        "batch_size = 16 # You can make this smaller (e.g., 8) if you run out of GPU memory\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=batch_size)\n",
        "\n",
        "print(\"\\nData preparation complete!\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of testing batches: {len(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3vLK_4Znq1F"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Initialize Model, Loss, and Optimizer\n",
        "\n",
        "# --- 1. Define Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 2. Define Model Hyperparameters ---\n",
        "vocab_size = tokenizer.vocab_size  # Get vocab size from the tokenizer\n",
        "num_classes = 2                    # 2 classes: positive and negative\n",
        "d_model = 256                      # SMALLER for faster training (paper used 512)\n",
        "h = 4                              # SMALLER (paper used 8)\n",
        "d_ff = 1024                        # SMALLER (paper used 2048)\n",
        "N = 3                              # SMALLER (paper used 6)\n",
        "dropout = 0.1\n",
        "pad_token_id = tokenizer.pad_token_id # Get the <PAD> token ID\n",
        "\n",
        "# --- 3. Initialize the Model ---\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    h=h,\n",
        "    d_ff=d_ff,\n",
        "    N=N,\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "# --- 4. Initialize Loss Function ---\n",
        "# We use CrossEntropyLoss because our model outputs raw logits\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 5. Initialize Optimizer ---\n",
        "# The paper used a custom learning rate, but standard Adam works well\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"Model, Loss, and Optimizer are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aiZWgJnWnsa6"
      },
      "outputs": [],
      "source": [
        "# Cell 6: The Training Loop\n",
        "import time\n",
        "\n",
        "num_epochs = 3 # Train for 3 epochs (full passes over the data)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        # 1. Get data and move to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # 2. Create the real padding mask\n",
        "        # We use the new function from Cell 2\n",
        "        mask = create_padding_mask(input_ids, pad_token_id).to(device)\n",
        "\n",
        "        # 3. Forward pass\n",
        "        optimizer.zero_grad() # Clear old gradients\n",
        "        outputs = model(input_ids, mask) # Get model predictions\n",
        "\n",
        "        # 4. Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 5. Backward pass and optimize\n",
        "        loss.backward() # Calculate gradients\n",
        "        optimizer.step() # Update model weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # --- End of Epoch ---\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradients needed for evaluation\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            mask = create_padding_mask(input_ids, pad_token_id).to(device)\n",
        "\n",
        "            outputs = model(input_ids, mask)\n",
        "\n",
        "            # Get the class with the highest score\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Training Complete! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J5W5SZfUUVe-"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Test on your own sentences\n",
        "\n",
        "def predict_sentiment(sentence):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # 1. Tokenize the sentence\n",
        "    inputs = tokenizer(sentence,\n",
        "                       return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "                       padding=\"max_length\",\n",
        "                       truncation=True,\n",
        "                       max_length=512)\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "    # 2. Create the padding mask\n",
        "    # We can just use the attention_mask from the tokenizer!\n",
        "    # But we need to reshape it for our model\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    mask = attention_mask.unsqueeze(1).unsqueeze(2) # [1, 1, 1, seq_len]\n",
        "\n",
        "    # 3. Get the prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, mask)\n",
        "\n",
        "    # 4. Get the final class\n",
        "    _, predicted_class = torch.max(outputs, 1)\n",
        "\n",
        "    if predicted_class.item() == 1:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\"\n",
        "\n",
        "# --- Try it out! ---\n",
        "print(predict_sentiment(\"This movie was absolutely fantastic! I loved it.\"))\n",
        "print(predict_sentiment(\"It was a complete waste of time. The plot was terrible.\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0DM3elBPT7GU2xqgoVhJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}